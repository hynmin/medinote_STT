"""
STT ì‹¤í–‰ ë©”ì¸ íŒŒì¼
"""
import argparse
from pathlib import Path
from models.stt.engine.whisper_engine import MedicalSTT, OpenAIWhisperSTT
from db.storage import init_db, save_transcript, save_summary
from models.stt.utils.metrics import compute_metrics, compute_rtf
from models.stt.core.config import STTConfig
from models.stt.pipelines.summarize import generate_summary
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()


def load_reference_text(args):
    """
    í‰ê°€ìš© ì°¸ì¡° í…ìŠ¤íŠ¸ ë¡œë“œ

    TODO: í–¥í›„ ì˜ë£Œ ìƒë‹´ í‰ê°€ ì§€í‘œë¥¼ ìž¬ì •ì˜í•  ë•Œ ì´ í•¨ìˆ˜ì™€ ê´€ë ¨ ì½”ë“œë¥¼ ìˆ˜ì •/ì‚­ì œ
    """
    ref_text = None
    if args.ref_file:
        try:
            with open(args.ref_file, "r", encoding="utf-8") as rf:
                ref_text = rf.read()
        except Exception as e:
            print(f"âš ï¸ Failed to read ref file: {e}")
    return ref_text


def main():
    parser = argparse.ArgumentParser(description="ì˜ë£Œ ìƒë‹´ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜")

    parser.add_argument( #cli í…ŒìŠ¤íŠ¸ìš©. data/audio/íŒŒì¼
        "audio_path",
        type=str,
        help="ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” ë””ë ‰í† ë¦¬"
    )
    
    parser.add_argument(
        "--model",
        type=str,
        choices=["fast", "balanced", "accurate"],
        default="fast",
        help="ì‚¬ìš©í•  ëª¨ë¸ (default: fast)"
    )

    parser.add_argument( #ê°œë°œë‹¨ê³„ wer/cer ê³„ì‚°ìš©
        "--ref-file",
        type=str,
        default=None,
        help="í‰ê°€ìš© ì°¸ì¡° í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ(UTF-8)"
    )
    
    parser.add_argument(
        "--no-noise-reduction",
        action="store_true",
        help="ë…¸ì´ì¦ˆ ì œê±° ë¹„í™œì„±í™” (ê¸°ë³¸: í™œì„±í™”)"
    )
    
    parser.add_argument(
        "--vad",
        action="store_true",
        help="VAD(Voice Activity Detection) ì‚¬ìš© - ë¬´ìŒ êµ¬ê°„ ì œê±°"
    )

    parser.add_argument(
        "--use-openai-api",
        type=str,
        nargs="?",
        const="whisper-1",
        default=None,
        choices=["whisper-1", "gpt-4o-transcribe", "gpt-4o-mini-transcribe"],
        help="OpenAI API ì‚¬ìš©. ëª¨ë¸ ì„ íƒ: whisper-1(ê¸°ë³¸), gpt-4o-transcribe, gpt-4o-mini-transcribe"
    )
    args = parser.parse_args()

    # STT ì—”ì§„ ì´ˆê¸°í™”
    if args.use_openai_api:
        # OpenAI API ì‚¬ìš©
        stt = OpenAIWhisperSTT(model=args.use_openai_api)
        print(f"ðŸŒ Using OpenAI API: {args.use_openai_api}")
    else:
        # ë¡œì»¬ Hugging Face ëª¨ë¸ ì‚¬ìš©
        stt = MedicalSTT(
            model_type=args.model,
            noise_reduction=not args.no_noise_reduction,
            use_vad=args.vad
        )

    # DB ì´ˆê¸°í™”
    db_path = STTConfig.DB_PATH
    init_db(db_path)
    
    audio_path = Path(args.audio_path)
    
    # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
    if audio_path.is_file():
        result = stt.transcribe(
            str(audio_path),
        )

        # ë³€í™˜ ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*50)
        print("ðŸ“„ ë³€í™˜ ê²°ê³¼:")
        print("="*50)
        print(result["text"])

        # DB ì €ìž¥ êµ¬ë¶„ í•„ìš”í•œì§€ í™•ì¸ í•„ìš”
        if True:
            # RTF ê³„ì‚°
            rtf_info = compute_rtf(result.get("processing_time", 0), result.get("audio_duration", 0))

            tid = save_transcript(
                result, # STT ê²°ê³¼ dict (audio_file, model, text í¬í•¨)
                result.get("processing_time"),
                result.get("audio_duration"),
                rtf_info.get("rtf"),
                not args.no_noise_reduction,
                db_path
            )
            print(f"ðŸ—„ï¸  Saved to DB: {db_path} (transcript_id={tid})")

            # RTF ì¶œë ¥
            audio_duration = result.get("audio_duration")
            if audio_duration and audio_duration > 0:
                print(f"\nâš¡ Performance")
                rtf_value = rtf_info['rtf']
                if rtf_value <= 1.0:
                    print(f"  RTF: {rtf_value:.4f} (ì‹¤ì‹œê°„ë³´ë‹¤ {1/rtf_value:.2f}ë°° ë¹ ë¦„)")
                else:
                    print(f"  RTF: {rtf_value:.4f} (ì‹¤ì‹œê°„ë³´ë‹¤ {rtf_value:.2f}ë°° ëŠë¦¼)")
                print(f"  ì²˜ë¦¬ ì‹œê°„: {result.get('processing_time', 0):.2f}ì´ˆ / ì˜¤ë””ì˜¤ ê¸¸ì´: {audio_duration:.2f}ì´ˆ")
            else:
                print(f"\nâš¡ Performance")
                print(f"  ì²˜ë¦¬ ì‹œê°„: {result.get('processing_time', 0):.2f}ì´ˆ (RTF ê³„ì‚° ë¶ˆê°€ - ì˜¤ë””ì˜¤ ê¸¸ì´ ì •ë³´ ì—†ìŒ)")

            # AI ìš”ì•½ ìƒì„± (í…ìŠ¤íŠ¸ê°€ ìžˆì„ ë•Œë§Œ)
            if result["text"].strip():
                print("\nðŸ¤– AI ìš”ì•½ ìƒì„± ì¤‘...")
                try:
                    summary_result = generate_summary(
                        transcript_text=result["text"],
                        model="gpt-4o-mini"
                    )

                    summary_id = save_summary(
                        transcript_id=tid,
                        chief_complaint=summary_result["chief_complaint"],
                        diagnosis=summary_result["diagnosis"],
                        recommendation=summary_result["recommendation"],
                        model=summary_result["model"],
                        summary_time=summary_result["summary_time"],
                        db_path=db_path
                    )

                    # í„°ë¯¸ë„ì— ìš”ì•½ ì¶œë ¥
                    print("\n" + "="*50)
                    print("AI ìš”ì•½")
                    print("="*50)
                    print(f"\n  ì¦ìƒ:")
                    print(f"  {summary_result['chief_complaint']}")
                    print(f"\n  ì§„ë‹¨:")
                    print(f"  {summary_result['diagnosis']}")
                    print(f"\n ì†Œê²¬:")
                    for line in summary_result['recommendation'].split('\n'):
                        if line.strip():
                            print(line)
                    print(f"\n ìš”ì•½ ìƒì„± ì‹œê°„: {summary_result['summary_time']}ì´ˆ (summary_id={summary_id})")

                except Exception as e:
                    print(f"âš ï¸  AI ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            else:
                print("\nâ­ï¸  í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìžˆì–´ AI ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

        # í‰ê°€ì§€í‘œ ê³„ì‚°/ì¶œë ¥/ì €ìž¥ (ì˜µì…˜)
        # TODO: í–¥í›„ ì˜ë£Œ ìƒë‹´ í‰ê°€ ì§€í‘œë¥¼ ìž¬ì •ì˜í•  ë•Œ ì´ ë¸”ë¡ì„ ìˆ˜ì •/ì‚­ì œ
        ref_text = load_reference_text(args)
        if ref_text:
            m = compute_metrics(ref_text, result.get("text", ""))
            print("\nðŸ“ Metrics")
            print(f"  WER: {m['wer']:.4f}  CER: {m['cer']:.4f}")

    else:
        print(f"âŒ Invalid audio file path: {audio_path}")


if __name__ == "__main__":
    main()
