"""
STT ì‹¤í–‰ ë©”ì¸ íŒŒì¼
"""
import argparse
from pathlib import Path
from models.stt.engine.hf_engine import HFWhisperSTT
from models.stt.engine.openai_engine import OpenAIWhisperSTT
from db.storage import init_db, save_transcript, save_summary
from models.stt.utils.metrics import compute_metrics, compute_rtf
from models.stt.core.config import STTConfig
from models.stt.pipelines.summarize import generate_summary
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()


def load_reference_text(args):
    """
    í‰ê°€ìš© ì°¸ì¡° í…ìŠ¤íŠ¸ ë¡œë“œ
    """
    ref_text = None
    if args.ref_file:
        try:
            with open(args.ref_file, "r", encoding="utf-8") as rf:
                ref_text = rf.read()
        except Exception as e:
            print(f"âš ï¸ Failed to read ref file: {e}")
    return ref_text


def main():
    parser = argparse.ArgumentParser(description="ì˜ë£Œ ìƒë‹´ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜")

    parser.add_argument( #cli í…ŒìŠ¤íŠ¸ìš©. data/audio/íŒŒì¼
        "audio_path",
        type=str,
        help="ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” ë””ë ‰í† ë¦¬"
    )  
    parser.add_argument( #ëª¨ë¸ ì„ íƒ
        "--model",
        type=str,
        choices=STTConfig.MODEL_CHOICES,
        default=STTConfig.DEFAULT_MODEL,
        help="ì‚¬ìš©í•  ëª¨ë¸ (default: fast)"
    )
    parser.add_argument( #ê°œë°œë‹¨ê³„ wer/cer ê³„ì‚°ìš©
        "--ref-file",
        type=str,
        default=None,
        help="í‰ê°€ìš© ì°¸ì¡° í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ(UTF-8)"
    )
    parser.add_argument( #ë…¸ì´ì¦ˆ ì œê±°
        "--no-noise-reduction",
        action="store_true",
        help="ë…¸ì´ì¦ˆ ì œê±° ë¹„í™œì„±í™” (ê¸°ë³¸: í™œì„±í™”, HF ëª¨ë¸ë§Œ ì ìš©)"
    )
    parser.add_argument( #ìŒì„±í™œë™ê°ì§€
        "--vad",
        action="store_true",
        help="VAD(Voice Activity Detection) ì‚¬ìš© (HF ëª¨ë¸ë§Œ ì ìš©)"
    )

    args = parser.parse_args()

    # STT ì—”ì§„ ì„ íƒ
    if STTConfig.is_api_model(args.model):
        # OpenAI API ëª¨ë¸
        stt = OpenAIWhisperSTT(model=args.model)
        print(f"Using OpenAI API: {args.model}")
    else:
        # HuggingFace ë¡œì»¬ ëª¨ë¸
        stt = HFWhisperSTT(
            model=args.model,
            noise_reduction=not args.no_noise_reduction,
            use_vad=args.vad
        )

    # í…Œì´ë¸” ì—†ìœ¼ë©´ ìƒì„±
    db_path = STTConfig.DB_PATH
    init_db(db_path)
    
    audio_path = Path(args.audio_path)
    
    # ìŒì„±íŒŒì¼ STT ì²˜ë¦¬
    if audio_path.is_file(): 
        result = stt.transcribe(   # ìŒì„±íŒŒì¼ STT ì²˜ë¦¬
            str(audio_path),
        )

        # ë³€í™˜ ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*50)
        print("ðŸ“„ ë³€í™˜ ê²°ê³¼:")
        print("="*50)
        print(result["text"])

        # RTF ê³„ì‚° ë° cli ì¶œë ¥
        rtf = compute_rtf(result.get("processing_time", 0), result.get("audio_duration", 0))
        audio_duration = result.get("audio_duration")
        file_size_mb = audio_path.stat().st_size / (1024 * 1024)  # bytes to MB

        print(f"\nâš¡ Performance")
        print(f"  íŒŒì¼ í¬ê¸°: {file_size_mb:.2f} MB")
        if audio_duration and audio_duration > 0:
            if rtf <= 1.0:
                print(f"  RTF: {rtf:.4f} (ì‹¤ì‹œê°„ë³´ë‹¤ {1/rtf:.2f}ë°° ë¹ ë¦„)")
            else:
                print(f"  RTF: {rtf:.4f} (ì‹¤ì‹œê°„ë³´ë‹¤ {rtf:.2f}ë°° ëŠë¦¼)")
            print(f"  ì²˜ë¦¬ ì‹œê°„: {result.get('processing_time', 0):.2f}ì´ˆ / ì˜¤ë””ì˜¤ ê¸¸ì´: {audio_duration:.2f}ì´ˆ")
        else:
            print(f"  ì²˜ë¦¬ ì‹œê°„: {result.get('processing_time', 0):.2f}ì´ˆ (RTF ê³„ì‚° ë¶ˆê°€ - ì˜¤ë””ì˜¤ ê¸¸ì´ ì •ë³´ ì—†ìŒ)")

        # STT ê²°ê³¼ DBì €ìž¥
        tid = save_transcript(
            result,
            result.get("processing_time"),
            result.get("audio_duration"),
            rtf,
            not args.no_noise_reduction,
            db_path
        )
        print(f"ðŸ—„ï¸  Saved to DB: {db_path} (transcript_id={tid})")

        # AI ìš”ì•½ ìƒì„±
        if result["text"].strip():
            print("\nðŸ¤– AI ìš”ì•½ ìƒì„± ì¤‘...")
            try:
                summary_result = generate_summary(  # ìš”ì•½ì •ë¦¬ ìƒì„±
                    transcript_text=result["text"],
                    model="gpt-4o-mini"
                )

                summary_id = save_summary(          # ìš”ì•½ì •ë¦¬ DB ì €ìž¥
                    transcript_id=tid,
                    chief_complaint=summary_result["chief_complaint"],
                    diagnosis=summary_result["diagnosis"],
                    recommendation=summary_result["recommendation"],
                    model=summary_result["model"],
                    summary_time=summary_result["summary_time"],
                    db_path=db_path
                )

                # í„°ë¯¸ë„ì— ìš”ì•½ ì¶œë ¥
                print("\n" + "="*50)
                print("AI ìš”ì•½")
                print("="*50)
                print(f"\n  ì¦ìƒ:")
                print(f"  {summary_result['chief_complaint']}")
                print(f"\n  ì§„ë‹¨:")
                print(f"  {summary_result['diagnosis']}")
                print(f"\n ì†Œê²¬:")
                for line in summary_result['recommendation'].split('\n'):
                    if line.strip():
                        print(line)
                print(f"\n ìš”ì•½ ìƒì„± ì‹œê°„: {summary_result['summary_time']}ì´ˆ (summary_id={summary_id})")

            except Exception as e:
                print(f"âš ï¸  AI ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
        else:
            print("\nâ­ï¸  í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìžˆì–´ AI ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

        # í‰ê°€ì§€í‘œ cli ì¶œë ¥ (ê°œë°œë‹¨ê³„)
        ref_text = load_reference_text(args)
        if ref_text:
            m = compute_metrics(ref_text, result.get("text", ""))
            print("\nðŸ“ Metrics")
            print(f"  WER: {m['wer']:.4f}  CER: {m['cer']:.4f}")
            print(f"  ì°¸ì¡° ê¸€ìžìˆ˜: {m['ref_chars']}  ì¸ì‹ ê¸€ìžìˆ˜: {m['hyp_chars']}")

    else:
        print(f"âŒ Invalid audio file path: {audio_path}")


if __name__ == "__main__":
    main()
