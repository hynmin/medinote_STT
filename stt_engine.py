"""
STT ì—”ì§„ í•µì‹¬ ë¡œì§
"""
from transformers import pipeline
import time
from pathlib import Path
from datetime import datetime
from config import STTConfig
import librosa
import numpy as np


class MedicalSTT:
    """ì˜ë£Œ ìƒë‹´ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” STT ì—”ì§„"""

    def __init__(self, model_type="fast", noise_reduction=False, use_vad=False):
        """
        Args:
            model_type: "fast", "balanced", "accurate" ì¤‘ í•˜ë‚˜
            noise_reduction: ë…¸ì´ì¦ˆ ì œê±° ì „ì²˜ë¦¬ ì‚¬ìš© ì—¬ë¶€
            use_vad: VAD(Voice Activity Detection) ì‚¬ìš© ì—¬ë¶€
        """
        self.model_type = model_type
        self.model_name = STTConfig.get_model(model_type)
        self.noise_reduction = noise_reduction
        self.use_vad = use_vad
        self.vad_model = None

        print(f"ğŸ”„ Loading {self.model_name}...")
        self.transcriber = pipeline(
            "automatic-speech-recognition",
            model=self.model_name,
            device=STTConfig.get_device(),
            return_timestamps=True
        )
        print(f"âœ… Model loaded successfully!")

        # VAD ëª¨ë¸ ë¡œë“œ (ì‚¬ìš© ì‹œì—ë§Œ)
        if self.use_vad:
            self._load_vad_model()
    
    def transcribe(self, audio_path, reference_text=None):
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜

        Args:
            audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            reference_text: í‰ê°€ìš© ì°¸ì¡° í…ìŠ¤íŠ¸ (ì œê³µ ì‹œ WER/CER ê³„ì‚°)

        Returns:
            dict: {
                "text": ë³€í™˜ëœ í…ìŠ¤íŠ¸,
                "audio_file": ì˜¤ë””ì˜¤ íŒŒì¼ëª…,
                "model": ì‚¬ìš©í•œ ëª¨ë¸,
                "processing_time": ì²˜ë¦¬ ì‹œê°„(ì´ˆ),
                "timestamp": ë³€í™˜ ì‹œê°,
                # reference_text ì œê³µ ì‹œ ì¶”ê°€:
                "metrics": {"wer": 0.05, "cer": 0.02}
            }
        """
        print(f"\nğŸ¤ Processing: {audio_path}")

        start_time = time.time()

        # ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì •
        audio_duration = self._get_audio_duration(audio_path)

        # ì˜¤ë””ì˜¤ ë¡œë“œí•˜ì—¬ ì—ë„ˆì§€ ë ˆë²¨ ì²´í¬
        y, sr = librosa.load(audio_path, sr=16000, mono=True)

        # 1) ë„ˆë¬´ ì§§ì€ ì˜¤ë””ì˜¤ ì²´í¬
        if audio_duration < STTConfig.MIN_AUDIO_DURATION:
            print(f"  ğŸ”‡ Audio too short ({audio_duration:.1f}s < {STTConfig.MIN_AUDIO_DURATION}s). Returning empty result.")
            processing_time = time.time() - start_time
            return {
                "text": "",
                "audio_file": Path(audio_path).name,
                "model": self.model_name,
                "processing_time": round(processing_time, 2),
                "audio_duration": round(audio_duration, 2),
                "rtf": round(processing_time / max(audio_duration, 0.001), 4)
            }

        # 2) ë¬´ìŒ ì²´í¬ (RMS ì—ë„ˆì§€)
        audio_rms = np.sqrt(np.mean(y**2))
        print(f"  ğŸ“Š Audio RMS energy: {audio_rms:.4f} (threshold: {STTConfig.SILENCE_RMS_THRESHOLD})")
        if audio_rms < STTConfig.SILENCE_RMS_THRESHOLD:
            print(f"  ğŸ”‡ Audio too quiet. Returning empty result.")
            processing_time = time.time() - start_time
            return {
                "text": "",
                "audio_file": Path(audio_path).name,
                "model": self.model_name,
                "processing_time": round(processing_time, 2),
                "audio_duration": round(audio_duration, 2),
                "rtf": round(processing_time / max(audio_duration, 0.001), 4)
            }

        # STT ìˆ˜í–‰ (ffmpeg ë¯¸ì„¤ì¹˜ ì‹œ librosaë¡œ ëŒ€ì²´ ë¡œë”©)
        generate_kwargs = {
            "language": STTConfig.LANGUAGE,
            "task": "transcribe"
        }

        # ë…¸ì´ì¦ˆ ì œê±° ì „ì²˜ë¦¬ (ì´ë¯¸ ë¡œë“œëœ ì˜¤ë””ì˜¤ ì‚¬ìš©)
        if self.noise_reduction:
            print("  ğŸ”§ Applying noise reduction...")
            y = self._apply_noise_reduction(y, sr)

        # VAD ì ìš© (ë¬´ìŒ êµ¬ê°„ ì œê±°)
        if self.use_vad:
            print("  ğŸ™ï¸ Applying VAD (removing silence)...")
            y = self._apply_vad(y, sr)

        audio_input = {"array": np.asarray(y), "sampling_rate": sr}

        # STT ìˆ˜í–‰ (librosaë¡œ ì´ë¯¸ ë¡œë“œí–ˆìœ¼ë¯€ë¡œ ì—ëŸ¬ ë³µêµ¬ ë¶ˆí•„ìš”)
        result = self.transcriber(audio_input, generate_kwargs=generate_kwargs)

        # DEBUG: Whisper ê²°ê³¼ êµ¬ì¡° í™•ì¸
        print(f"\nğŸ” DEBUG - Whisper result keys: {result.keys()}")
        if "chunks" in result and result["chunks"]:
            print(f"ğŸ” DEBUG - First chunk sample: {result['chunks'][0]}")
            print(f"ğŸ” DEBUG - Total chunks: {len(result['chunks'])}")

        processing_time = time.time() - start_time

        # ê²°ê³¼ ì •ë¦¬
        output = {
            "text": result["text"],
            "audio_file": Path(audio_path).name,
            "model": self.model_name,
            "processing_time": round(processing_time, 2),
            "audio_duration": round(audio_duration, 2),
            "timestamp": datetime.now().isoformat(),
            "segments": result.get("chunks", [])  # Whisper segments í¬í•¨ (ì‹ ë¢°ë„ ê³„ì‚°ìš©)
        }

        # í‰ê°€ ì§€í‘œ ê³„ì‚° (ì˜µì…˜)
        if reference_text:
            from dev_metrics import compute_metrics
            metrics = compute_metrics(reference_text, output["text"])
            output["metrics"] = metrics
            print(f"ğŸ“ WER: {metrics['wer']:.2%}, CER: {metrics['cer']:.2%}")

        print(f"âœ… Done in {processing_time:.2f}s")
        print(f"ğŸ“ Result: {output['text'][:100]}...")

        return output

    def _get_audio_duration(self, audio_path):
        """ì˜¤ë””ì˜¤ íŒŒì¼ ê¸¸ì´ë¥¼ ì´ˆ ë‹¨ìœ„ë¡œ ë°˜í™˜"""
        y, sr = librosa.load(audio_path, sr=None)
        return librosa.get_duration(y=y, sr=sr)

    def _apply_noise_reduction(self, y, sr):
        """ë…¸ì´ì¦ˆ ì œê±° ì „ì²˜ë¦¬"""
        import noisereduce as nr
        return nr.reduce_noise(y=y, sr=sr)

    def _load_vad_model(self):
        """Silero VAD ëª¨ë¸ ë¡œë“œ"""
        try:
            import torch
            print("  ğŸ”„ Loading Silero VAD model...")
            self.vad_model, utils = torch.hub.load(
                repo_or_dir='snakers4/silero-vad',
                model='silero_vad',
                force_reload=False,
                onnx=False
            )
            self.get_speech_timestamps = utils[0]
            print("  âœ… VAD model loaded successfully!")
        except Exception as e:
            print(f"  âš ï¸ Failed to load VAD model: {e}")
            self.use_vad = False
            self.vad_model = None

    def _apply_vad(self, y, sr):
        """VADë¥¼ ì ìš©í•˜ì—¬ ìŒì„± êµ¬ê°„ë§Œ ì¶”ì¶œ"""
        if self.vad_model is None:
            return y

        try:
            import torch

            # numpy arrayë¥¼ torch tensorë¡œ ë³€í™˜
            audio_tensor = torch.from_numpy(y).float()

            # ìŒì„± êµ¬ê°„ ê°ì§€
            speech_timestamps = self.get_speech_timestamps(
                audio_tensor,
                self.vad_model,
                sampling_rate=sr,
                threshold=0.5,
                min_speech_duration_ms=250,
                min_silence_duration_ms=100
            )

            if not speech_timestamps:
                print("  âš ï¸ No speech detected by VAD, using original audio")
                return y

            # ìŒì„± êµ¬ê°„ë§Œ ì¶”ì¶œí•˜ì—¬ ì—°ê²°
            speech_segments = []
            for segment in speech_timestamps:
                start = segment['start']
                end = segment['end']
                speech_segments.append(y[start:end])

            # ëª¨ë“  ìŒì„± êµ¬ê°„ ì—°ê²°
            vad_audio = np.concatenate(speech_segments) if speech_segments else y

            original_duration = len(y) / sr
            vad_duration = len(vad_audio) / sr
            print(f"  ğŸ“Š VAD: {original_duration:.1f}s â†’ {vad_duration:.1f}s ({(1 - vad_duration/original_duration)*100:.1f}% reduced)")

            return vad_audio

        except Exception as e:
            print(f"  âš ï¸ VAD failed: {e}, using original audio")
            return y

    def get_model_info(self):
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "model_type": self.model_type,
            "model_name": self.model_name,
            "device": "GPU" if STTConfig.get_device() == 0 else "CPU",
            "language": STTConfig.LANGUAGE,
            "noise_reduction": self.noise_reduction,
            "use_vad": self.use_vad
        }


# ê°„ë‹¨í•œ ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    stt = MedicalSTT(model_type="fast")
    result = stt.transcribe("audio.mp3")

    print("\nğŸ“‹ Model Info:")
    print(stt.get_model_info())
