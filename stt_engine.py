"""
STT ì—”ì§„ í•µì‹¬ ë¡œì§
"""
from transformers import pipeline
import time
import json
from pathlib import Path
from datetime import datetime
from config import STTConfig
import librosa
import numpy as np
import os

try:
    from pyannote.audio import Pipeline
    DIARIZATION_AVAILABLE = True
except ImportError:
    DIARIZATION_AVAILABLE = False
    print("âš ï¸ Warning: pyannote.audio not installed. Speaker diarization disabled.")


class MedicalSTT:
    """ì˜ë£Œ ìƒë‹´ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” STT ì—”ì§„"""

    def __init__(self, model_type="fast", enable_diarization=False, noise_reduction=False, vad_filter=False):
        """
        Args:
            model_type: "fast", "balanced", "accurate" ì¤‘ í•˜ë‚˜
            enable_diarization: í™”ì ë¶„ë¦¬ ì‚¬ìš© ì—¬ë¶€
            noise_reduction: ë…¸ì´ì¦ˆ ì œê±° ì „ì²˜ë¦¬ ì‚¬ìš© ì—¬ë¶€
            vad_filter: VAD (Voice Activity Detection) í•„í„° ì‚¬ìš© ì—¬ë¶€
        """
        self.model_type = model_type
        self.model_name = STTConfig.get_model(model_type)
        self.enable_diarization = enable_diarization and DIARIZATION_AVAILABLE
        self.noise_reduction = noise_reduction
        self.vad_filter = vad_filter

        print(f"ğŸ”„ Loading {self.model_name}...")
        self.transcriber = pipeline(
            "automatic-speech-recognition",
            model=self.model_name,
            device=STTConfig.get_device(),
            return_timestamps=True  # íƒ€ì„ìŠ¤íƒ¬í”„ í•„ìˆ˜ (í™”ì ë¶„ë¦¬ìš©)
        )
        print(f"âœ… Model loaded successfully!")

        # í™”ì ë¶„ë¦¬ ëª¨ë¸ ë¡œë“œ (ì„ íƒì )
        self.diarization_pipeline = None
        if self.enable_diarization:
            try:
                print("ğŸ”„ Loading speaker diarization model...")
                hf_token = os.getenv("HF_TOKEN")
                self.diarization_pipeline = Pipeline.from_pretrained(
                    "pyannote/speaker-diarization-3.1",
                    token=hf_token
                )
                print("âœ… Diarization model loaded!")
            except Exception as e:
                print(f"âš ï¸ Diarization loading failed: {e}")
                print("   Continuing without speaker separation...")
                self.enable_diarization = False
    
    def transcribe(self, audio_path, save_result=True, reference_text=None):
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (í™”ì ë¶„ë¦¬ ì˜µì…˜ í¬í•¨)

        Args:
            audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            save_result: ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥í• ì§€ ì—¬ë¶€
            reference_text: í‰ê°€ìš© ì°¸ì¡° í…ìŠ¤íŠ¸ (ì œê³µ ì‹œ WER/CER ê³„ì‚°)

        Returns:
            dict: {
                "text": ë³€í™˜ëœ í…ìŠ¤íŠ¸,
                "audio_file": ì˜¤ë””ì˜¤ íŒŒì¼ëª…,
                "model": ì‚¬ìš©í•œ ëª¨ë¸,
                "processing_time": ì²˜ë¦¬ ì‹œê°„(ì´ˆ),
                "timestamp": ë³€í™˜ ì‹œê°,

                # í™”ì ë¶„ë¦¬ í™œì„±í™” ì‹œ ì¶”ê°€:
                "segments": [...],  # í™”ìë³„ ì„¸ê·¸ë¨¼íŠ¸
                "num_speakers": 2,  # í™”ì ìˆ˜

                # reference_text ì œê³µ ì‹œ ì¶”ê°€:
                "metrics": {"wer": 0.05, "cer": 0.02}
            }
        """
        if self.enable_diarization:
            return self._transcribe_with_diarization(audio_path, save_result, reference_text)
        else:
            return self._transcribe_simple(audio_path, save_result, reference_text)

    def _transcribe_simple(self, audio_path, save_result=True, reference_text=None):
        """í™”ì ë¶„ë¦¬ ì—†ì´ ë‹¨ìˆœ ë³€í™˜"""
        print(f"\nğŸ¤ Processing: {audio_path}")

        start_time = time.time()

        # ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì •
        audio_duration = self._get_audio_duration(audio_path)

        # ì˜¤ë””ì˜¤ ë¡œë“œí•˜ì—¬ ì—ë„ˆì§€ ë ˆë²¨ ì²´í¬
        y, sr = librosa.load(audio_path, sr=16000, mono=True)

        # 1) ë„ˆë¬´ ì§§ì€ ì˜¤ë””ì˜¤ ì²´í¬
        if audio_duration < STTConfig.MIN_AUDIO_DURATION:
            print(f"  ğŸ”‡ Audio too short ({audio_duration:.1f}s < {STTConfig.MIN_AUDIO_DURATION}s). Returning empty result.")
            processing_time = time.time() - start_time
            return {
                "text": "",
                "audio_file": Path(audio_path).name,
                "model": self.model_name,
                "processing_time": round(processing_time, 2),
                "audio_duration": round(audio_duration, 2),
                "rtf": round(processing_time / max(audio_duration, 0.001), 4),
                "num_speakers": 0
            }

        # 2) ë¬´ìŒ ì²´í¬ (RMS ì—ë„ˆì§€)
        audio_rms = np.sqrt(np.mean(y**2))
        print(f"  ğŸ“Š Audio RMS energy: {audio_rms:.4f} (threshold: {STTConfig.SILENCE_RMS_THRESHOLD})")
        if audio_rms < STTConfig.SILENCE_RMS_THRESHOLD:
            print(f"  ğŸ”‡ Audio too quiet. Returning empty result.")
            processing_time = time.time() - start_time
            return {
                "text": "",
                "audio_file": Path(audio_path).name,
                "model": self.model_name,
                "processing_time": round(processing_time, 2),
                "audio_duration": round(audio_duration, 2),
                "rtf": round(processing_time / max(audio_duration, 0.001), 4),
                "num_speakers": 0
            }

        # STT ìˆ˜í–‰ (ffmpeg ë¯¸ì„¤ì¹˜ ì‹œ librosaë¡œ ëŒ€ì²´ ë¡œë”©)
        generate_kwargs = {
            "language": STTConfig.LANGUAGE,
            "task": "transcribe"
        }

        # VAD í•„í„° ì„¤ì •
        if self.vad_filter:
            print("  ğŸ¯ Using VAD filter (Voice Activity Detection)...")

        # ë…¸ì´ì¦ˆ ì œê±° ì „ì²˜ë¦¬ (ì´ë¯¸ ë¡œë“œëœ ì˜¤ë””ì˜¤ ì‚¬ìš©)
        if self.noise_reduction:
            print("  ğŸ”§ Applying noise reduction...")
            y = self._apply_noise_reduction(y, sr)
            audio_input = {"array": np.asarray(y), "sampling_rate": sr}
        else:
            audio_input = {"array": np.asarray(y), "sampling_rate": sr}

        try:
            result = self.transcriber(audio_input, generate_kwargs=generate_kwargs)
        except Exception as e:
            msg = str(e).lower()
            if "ffmpeg" in msg or "torchcodec" in msg or "libtorchcodec" in msg:
                y, sr = librosa.load(audio_path, sr=16000, mono=True)
                if self.noise_reduction:
                    y = self._apply_noise_reduction(y, sr)
                audio_input = {"array": np.asarray(y), "sampling_rate": sr}
                result = self.transcriber(audio_input, generate_kwargs=generate_kwargs)
            else:
                raise

        processing_time = time.time() - start_time

        # ê²°ê³¼ ì •ë¦¬
        output = {
            "text": result["text"],
            "audio_file": Path(audio_path).name,
            "model": self.model_name,
            "processing_time": round(processing_time, 2),
            "audio_duration": round(audio_duration, 2),
            "timestamp": datetime.now().isoformat()
        }

        # í‰ê°€ ì§€í‘œ ê³„ì‚° (ì˜µì…˜)
        if reference_text:
            from metrics import compute_metrics
            metrics = compute_metrics(reference_text, output["text"])
            output["metrics"] = metrics
            print(f"ğŸ“ WER: {metrics['wer']:.2%}, CER: {metrics['cer']:.2%}")

        print(f"âœ… Done in {processing_time:.2f}s")
        print(f"ğŸ“ Result: {output['text'][:100]}...")

        # ê²°ê³¼ ì €ì¥
        if save_result:
            self._save_result(output)

        return output

    def _transcribe_with_diarization(self, audio_path, save_result=True, reference_text=None):
        """í™”ì ë¶„ë¦¬ë¥¼ í¬í•¨í•œ ë³€í™˜"""
        print(f"\nğŸ¤ Processing with speaker separation: {audio_path}")
        start_time = time.time()

        # ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì •
        audio_duration = self._get_audio_duration(audio_path)

        # 1ë‹¨ê³„: í™”ì ë¶„ë¦¬ (ëˆ„ê°€ ì–¸ì œ ë§í–ˆëŠ”ì§€)
        speaker_segments = []
        if self.diarization_pipeline:
            print("  ğŸ“Š Step 1/2: Detecting speakers...")
            try:
                diarization = self.diarization_pipeline(audio_path)
                for turn, _, speaker in diarization.itertracks(yield_label=True):
                    speaker_segments.append({
                        "speaker": speaker,
                        "start": turn.start,
                        "end": turn.end
                    })
                print(f"  âœ… Found {len(set(s['speaker'] for s in speaker_segments))} speakers")
            except Exception as e:
                msg = str(e).lower()
                if "ffmpeg" in msg:
                    print("  âš ï¸ FFmpeg not found. Skipping diarization.")
                else:
                    print(f"  âš ï¸ Diarization failed: {e}")
                speaker_segments = [{
                    "speaker": "SPEAKER_00",
                    "start": 0.0,
                    "end": 999999.0
                }]
        else:
            speaker_segments = [{
                "speaker": "SPEAKER_00",
                "start": 0.0,
                "end": 999999.0
            }]

        # 2ë‹¨ê³„: ìŒì„± ì¸ì‹ (ë­ë¼ê³  ë§í–ˆëŠ”ì§€)
        print("  ğŸ¯ Step 2/2: Transcribing speech...")
        generate_kwargs = {
            "language": STTConfig.LANGUAGE,
            "task": "transcribe"
        }

        # VAD í•„í„° ì„¤ì •
        if self.vad_filter:
            generate_kwargs["vad_filter"] = True
            generate_kwargs["chunk_length_s"] = 30
            print("  ğŸ¯ Using VAD filter...")

        # ë…¸ì´ì¦ˆ ì œê±° ì „ì²˜ë¦¬
        if self.noise_reduction:
            audio_input = self._preprocess_audio(audio_path)
        else:
            audio_input = audio_path

        try:
            transcription = self.transcriber(audio_input, generate_kwargs=generate_kwargs)
        except Exception as e:
            msg = str(e).lower()
            if "ffmpeg" in msg or "torchcodec" in msg or "libtorchcodec" in msg:
                y, sr = librosa.load(audio_path, sr=16000, mono=True)
                if self.noise_reduction:
                    y = self._apply_noise_reduction(y, sr)
                audio_input = {"array": np.asarray(y), "sampling_rate": sr}
                transcription = self.transcriber(audio_input, generate_kwargs=generate_kwargs)
            else:
                raise

        # 3ë‹¨ê³„: í™”ìì™€ í…ìŠ¤íŠ¸ ë§¤ì¹­
        result_segments = []

        # Whisperì˜ chunks (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ í…ìŠ¤íŠ¸)
        if "chunks" in transcription:
            chunks = transcription["chunks"]
        else:
            chunks = [{
                "text": transcription["text"],
                "timestamp": (0.0, None)
            }]

        for chunk in chunks:
            text = chunk["text"].strip()
            if not text:
                continue

            chunk_start = chunk["timestamp"][0] if chunk["timestamp"][0] else 0.0
            chunk_end = chunk["timestamp"][1] if chunk["timestamp"][1] else chunk_start + 1.0

            # ì´ í…ìŠ¤íŠ¸ë¥¼ ë§í•œ í™”ì ì°¾ê¸°
            speaker = self._find_speaker(chunk_start, chunk_end, speaker_segments)

            result_segments.append({
                "speaker": speaker,
                "text": text,
                "start": round(chunk_start, 2),
                "end": round(chunk_end, 2)
            })

        processing_time = time.time() - start_time

        # ê²°ê³¼ ì •ë¦¬
        output = {
            "text": transcription["text"],
            "segments": result_segments,
            "num_speakers": len(set(s["speaker"] for s in result_segments)),
            "audio_file": Path(audio_path).name,
            "model": self.model_name,
            "processing_time": round(processing_time, 2),
            "audio_duration": round(audio_duration, 2),
            "timestamp": datetime.now().isoformat()
        }

        # í‰ê°€ ì§€í‘œ ê³„ì‚° (ì˜µì…˜)
        if reference_text:
            from metrics import compute_metrics
            metrics = compute_metrics(reference_text, output["text"])
            output["metrics"] = metrics
            print(f"ğŸ“ WER: {metrics['wer']:.2%}, CER: {metrics['cer']:.2%}")

        print(f"âœ… Transcription complete!")
        print(f"   ğŸ’¬ {len(result_segments)} segments from {output['num_speakers']} speakers")

        # ê²°ê³¼ ì €ì¥
        if save_result:
            self._save_result(output)

        return output

    def _find_speaker(self, chunk_start, chunk_end, speaker_segments):
        """í…ìŠ¤íŠ¸ êµ¬ê°„ì— í•´ë‹¹í•˜ëŠ” í™”ì ì°¾ê¸°"""
        chunk_mid = (chunk_start + chunk_end) / 2

        for segment in speaker_segments:
            if segment["start"] <= chunk_mid <= segment["end"]:
                return segment["speaker"]

        # ëª» ì°¾ìœ¼ë©´ ê°€ì¥ ê°€ê¹Œìš´ í™”ì
        if speaker_segments:
            return speaker_segments[0]["speaker"]

        return "SPEAKER_00"

    def _get_audio_duration(self, audio_path):
        """ì˜¤ë””ì˜¤ íŒŒì¼ ê¸¸ì´(ì´ˆ) ì¸¡ì •"""
        try:
            import soundfile as sf
            with sf.SoundFile(audio_path) as f:
                return len(f) / f.samplerate
        except Exception:
            # soundfile ì‹¤íŒ¨ ì‹œ librosaë¡œ ëŒ€ì²´
            try:
                y, sr = librosa.load(audio_path, sr=None)
                return len(y) / sr
            except Exception:
                return 0.0

    def _preprocess_audio(self, audio_path):
        """ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ (ë…¸ì´ì¦ˆ ì œê±°)"""
        print("  ğŸ”§ Applying noise reduction...")
        y, sr = librosa.load(audio_path, sr=16000, mono=True)
        y = self._apply_noise_reduction(y, sr)
        return {"array": np.asarray(y), "sampling_rate": sr}

    def _apply_noise_reduction(self, audio_array, sample_rate):
        """ë…¸ì´ì¦ˆ ì œê±° ì ìš©"""
        try:
            import noisereduce as nr
            # ë…¸ì´ì¦ˆ í”„ë¡œíŒŒì¼ ìë™ ì¶”ì • ë° ì œê±°
            reduced = nr.reduce_noise(
                y=audio_array,
                sr=sample_rate,
                stationary=True,  # ë°°ê²½ì†ŒìŒì´ ì¼ì •í•œ ê²½ìš°
                prop_decrease=1.0  # ë…¸ì´ì¦ˆ ê°ì†Œ ì •ë„ (0.0~1.0)
            )
            return reduced
        except ImportError:
            print("  âš ï¸ noisereduce not installed. Skipping noise reduction.")
            return audio_array
        except Exception as e:
            print(f"  âš ï¸ Noise reduction failed: {e}")
            return audio_array
    
    def transcribe_batch(self, audio_paths, save_result=True):
        """
        ì—¬ëŸ¬ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì¼ê´„ ë³€í™˜
        
        Args:
            audio_paths: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            save_result: ê²°ê³¼ë¥¼ ì €ì¥í• ì§€ ì—¬ë¶€
            
        Returns:
            list: ë³€í™˜ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        print(f"\nğŸ“¦ Batch processing {len(audio_paths)} files...")
        
        for i, audio_path in enumerate(audio_paths, 1):
            print(f"\n[{i}/{len(audio_paths)}]", end=" ")
            result = self.transcribe(audio_path, save_result=False)
            results.append(result)
        
        # ì¼ê´„ ì €ì¥
        if save_result:
            self._save_batch_result(results)
        
        return results
    
    def _save_result(self, result):
        """ë‹¨ì¼ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        audio_name = Path(result["audio_file"]).stem
        suffix = "_with_speakers" if "segments" in result else "_transcription"
        output_path = Path(STTConfig.OUTPUT_DIR) / f"{audio_name}{suffix}.json"

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ Saved to: {output_path}")
    
    def _save_batch_result(self, results):
        """ë°°ì¹˜ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = Path(STTConfig.OUTPUT_DIR) / f"batch_transcription_{timestamp}.json"
        
        batch_summary = {
            "total_files": len(results),
            "model": self.model_name,
            "timestamp": datetime.now().isoformat(),
            "results": results
        }
        
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(batch_summary, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ Batch results saved to: {output_path}")
    
    def print_conversation(self, result):
        """ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥ (í™”ì ë¶„ë¦¬ ê²°ê³¼ìš©)"""
        if "segments" not in result:
            print("\nâš ï¸ No speaker segments found (diarization was not enabled)")
            return

        print("\n" + "="*60)
        print("ğŸ“‹ ëŒ€í™” ë‚´ìš©")
        print("="*60)

        for segment in result["segments"]:
            speaker = segment["speaker"]
            text = segment["text"]
            time_info = f"[{segment['start']:.1f}s - {segment['end']:.1f}s]"

            print(f"\n{speaker} {time_info}")
            print(f"  {text}")

        print("\n" + "="*60)

    def get_model_info(self):
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "model_type": self.model_type,
            "model_name": self.model_name,
            "device": "GPU" if STTConfig.get_device() == 0 else "CPU",
            "language": STTConfig.LANGUAGE,
            "diarization_enabled": self.enable_diarization
        }


# ê°„ë‹¨í•œ ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # í™”ì ë¶„ë¦¬ ì—†ì´ ì‚¬ìš©
    stt = MedicalSTT(model_type="fast")
    result = stt.transcribe("audio.mp3")

    # í™”ì ë¶„ë¦¬ í¬í•¨
    # stt = MedicalSTT(model_type="fast", enable_diarization=True)
    # result = stt.transcribe("audio.mp3")
    # stt.print_conversation(result)

    print("\nğŸ“‹ Model Info:")
    print(stt.get_model_info())
