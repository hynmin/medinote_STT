"""
STT ì—”ì§„ í•µì‹¬ ë¡œì§
"""
from transformers import pipeline
import time
import json
from pathlib import Path
from datetime import datetime
from config import STTConfig
import librosa
import numpy as np
import os


class MedicalSTT:
    """ì˜ë£Œ ìƒë‹´ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” STT ì—”ì§„"""

    def __init__(self, model_type="fast", noise_reduction=False):
        """
        Args:
            model_type: "fast", "balanced", "accurate" ì¤‘ í•˜ë‚˜
            noise_reduction: ë…¸ì´ì¦ˆ ì œê±° ì „ì²˜ë¦¬ ì‚¬ìš© ì—¬ë¶€
        """
        self.model_type = model_type
        self.model_name = STTConfig.get_model(model_type)
        self.noise_reduction = noise_reduction

        print(f"ğŸ”„ Loading {self.model_name}...")
        self.transcriber = pipeline(
            "automatic-speech-recognition",
            model=self.model_name,
            device=STTConfig.get_device(),
            return_timestamps=True
        )
        print(f"âœ… Model loaded successfully!")
    
    def transcribe(self, audio_path, save_result=True, reference_text=None):
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜

        Args:
            audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            save_result: ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥í• ì§€ ì—¬ë¶€
            reference_text: í‰ê°€ìš© ì°¸ì¡° í…ìŠ¤íŠ¸ (ì œê³µ ì‹œ WER/CER ê³„ì‚°)

        Returns:
            dict: {
                "text": ë³€í™˜ëœ í…ìŠ¤íŠ¸,
                "audio_file": ì˜¤ë””ì˜¤ íŒŒì¼ëª…,
                "model": ì‚¬ìš©í•œ ëª¨ë¸,
                "processing_time": ì²˜ë¦¬ ì‹œê°„(ì´ˆ),
                "timestamp": ë³€í™˜ ì‹œê°,
                # reference_text ì œê³µ ì‹œ ì¶”ê°€:
                "metrics": {"wer": 0.05, "cer": 0.02}
            }
        """
        print(f"\nğŸ¤ Processing: {audio_path}")

        start_time = time.time()

        # ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì •
        audio_duration = self._get_audio_duration(audio_path)

        # ì˜¤ë””ì˜¤ ë¡œë“œí•˜ì—¬ ì—ë„ˆì§€ ë ˆë²¨ ì²´í¬
        y, sr = librosa.load(audio_path, sr=16000, mono=True)

        # 1) ë„ˆë¬´ ì§§ì€ ì˜¤ë””ì˜¤ ì²´í¬
        if audio_duration < STTConfig.MIN_AUDIO_DURATION:
            print(f"  ğŸ”‡ Audio too short ({audio_duration:.1f}s < {STTConfig.MIN_AUDIO_DURATION}s). Returning empty result.")
            processing_time = time.time() - start_time
            return {
                "text": "",
                "audio_file": Path(audio_path).name,
                "model": self.model_name,
                "processing_time": round(processing_time, 2),
                "audio_duration": round(audio_duration, 2),
                "rtf": round(processing_time / max(audio_duration, 0.001), 4)
            }

        # 2) ë¬´ìŒ ì²´í¬ (RMS ì—ë„ˆì§€)
        audio_rms = np.sqrt(np.mean(y**2))
        print(f"  ğŸ“Š Audio RMS energy: {audio_rms:.4f} (threshold: {STTConfig.SILENCE_RMS_THRESHOLD})")
        if audio_rms < STTConfig.SILENCE_RMS_THRESHOLD:
            print(f"  ğŸ”‡ Audio too quiet. Returning empty result.")
            processing_time = time.time() - start_time
            return {
                "text": "",
                "audio_file": Path(audio_path).name,
                "model": self.model_name,
                "processing_time": round(processing_time, 2),
                "audio_duration": round(audio_duration, 2),
                "rtf": round(processing_time / max(audio_duration, 0.001), 4)
            }

        # STT ìˆ˜í–‰ (ffmpeg ë¯¸ì„¤ì¹˜ ì‹œ librosaë¡œ ëŒ€ì²´ ë¡œë”©)
        generate_kwargs = {
            "language": STTConfig.LANGUAGE,
            "task": "transcribe"
        }

        # ë…¸ì´ì¦ˆ ì œê±° ì „ì²˜ë¦¬ (ì´ë¯¸ ë¡œë“œëœ ì˜¤ë””ì˜¤ ì‚¬ìš©)
        if self.noise_reduction:
            print("  ğŸ”§ Applying noise reduction...")
            y = self._apply_noise_reduction(y, sr)
            audio_input = {"array": np.asarray(y), "sampling_rate": sr}
        else:
            audio_input = {"array": np.asarray(y), "sampling_rate": sr}

        # STT ìˆ˜í–‰ (librosaë¡œ ì´ë¯¸ ë¡œë“œí–ˆìœ¼ë¯€ë¡œ ì—ëŸ¬ ë³µêµ¬ ë¶ˆí•„ìš”)
        result = self.transcriber(audio_input, generate_kwargs=generate_kwargs)

        processing_time = time.time() - start_time

        # ê²°ê³¼ ì •ë¦¬
        output = {
            "text": result["text"],
            "audio_file": Path(audio_path).name,
            "model": self.model_name,
            "processing_time": round(processing_time, 2),
            "audio_duration": round(audio_duration, 2),
            "timestamp": datetime.now().isoformat()
        }

        # í‰ê°€ ì§€í‘œ ê³„ì‚° (ì˜µì…˜)
        if reference_text:
            from metrics import compute_metrics
            metrics = compute_metrics(reference_text, output["text"])
            output["metrics"] = metrics
            print(f"ğŸ“ WER: {metrics['wer']:.2%}, CER: {metrics['cer']:.2%}")

        print(f"âœ… Done in {processing_time:.2f}s")
        print(f"ğŸ“ Result: {output['text'][:100]}...")

        # ê²°ê³¼ ì €ì¥
        if save_result:
            self._save_result(output)

        return output

    def get_model_info(self):
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "model_type": self.model_type,
            "model_name": self.model_name,
            "device": "GPU" if STTConfig.get_device() == 0 else "CPU",
            "language": STTConfig.LANGUAGE,
            "noise_reduction": self.noise_reduction
        }


# ê°„ë‹¨í•œ ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    stt = MedicalSTT(model_type="fast")
    result = stt.transcribe("audio.mp3")

    print("\nğŸ“‹ Model Info:")
    print(stt.get_model_info())
